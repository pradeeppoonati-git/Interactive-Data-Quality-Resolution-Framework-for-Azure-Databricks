{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c48588-5537-4856-8f67-059364177477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%run ../00_Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4b9364-b65b-4880-8c67-69ba81d81a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the Brightspace sample data\n",
    "file_path = \"/Workspace/Users/pradeep.ponati@gmail.com/samplefiles/brightspace_sample_data.csv\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c861d56-c2c6-446e-8845-bcc3e3d6bfea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV into DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4aeaf3-0977-4f95-8ecc-a5e76de7fd0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show basic info like count of records, also some sample data \n",
    "print(f\"‚úì Loaded {df.count()} records\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa074ac-69c9-4fa8-a37a-b0e00781b40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f5173f-a34f-48eb-9967-d2bdb0170f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use your ADLS Gen2 storage account\n",
    "storage_account = \"streamingstorageb\"\n",
    "container = \"bronze\"  # We'll use a container called 'bronze'\n",
    "\n",
    "# Define Bronze table path in ADLS Gen2\n",
    "bronze_path = BRONZE_BRIGHTSPACE\n",
    "\n",
    "print(f\"Target path: {bronze_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2e4a34-5797-449c-8d13-0fdb65d6105e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add metadata columns\n",
    "from pyspark.sql.functions import current_timestamp, current_date, lit, md5, concat_ws\n",
    "\n",
    "df_bronze = df \\\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"_ingestion_date\", current_date()) \\\n",
    "    .withColumn(\"_source_system\", lit(\"brightspace_lms\")) \\\n",
    "    .withColumn(\"_record_hash\", md5(concat_ws(\"|\", *df.columns)))\n",
    "\n",
    "print(f\"\\nüìù Writing to ADLS Gen2...\")\n",
    "print(f\"   Storage Account: {storage_account}\")\n",
    "print(f\"   Container: {container}\")\n",
    "print(f\"   Records: {df_bronze.count()}\")\n",
    "\n",
    "try:\n",
    "    # Write to Delta Lake on ADLS Gen2\n",
    "    df_bronze.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(bronze_path)\n",
    "    \n",
    "    print(f\"\\n‚úì Bronze table created successfully!\")\n",
    "    print(f\"‚úì Location: {bronze_path}\")\n",
    "    \n",
    "    # Verify\n",
    "    bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "    print(f\"‚úì Verification: Read back {bronze_df.count()} records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Error: {str(e)}\")\n",
    "    print(\"\\nThis might mean:\")\n",
    "    print(\"1. Container 'bronze' doesn't exist in your storage account\")\n",
    "    print(\"2. Authentication not configured\")\n",
    "    print(\"\\nLet me know what error you see and we'll fix it!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Test_Brightspace_Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
